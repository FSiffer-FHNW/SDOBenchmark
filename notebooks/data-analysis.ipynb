{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import collections\n",
    "import intervaltree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(\"~/data/flare-benchmark/\")\n",
    "raw_dir = os.path.join(data_dir, \"raw\")\n",
    "\n",
    "events_file = os.path.join(raw_dir, \"events_2012-01-01T00:00:00_2018-01-01T00:00:00.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(events_file, \"r\") as f:\n",
    "    events = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWPC flares: 10954\n",
      "SPoCA regions: 76983\n",
      "SPoCA regions (grouped): 11246\n"
     ]
    }
   ],
   "source": [
    "def dtf(raw_date):\n",
    "    return dt.datetime.strptime(raw_date, \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "swpc_flares = list(sorted(\n",
    "    (event for event in events if event[\"event_type\"] == \"FL\" and event[\"frm_name\"] == \"SWPC\"),\n",
    "    key=lambda event: (dtf(event[\"event_starttime\"]), dtf(event[\"event_peaktime\"]), dtf(event[\"event_endtime\"]))\n",
    "))\n",
    "print(\"SWPC flares:\", len(swpc_flares))\n",
    "\n",
    "spoca_regions = list(sorted(\n",
    "    (event for event in events if event[\"event_type\"] == \"AR\" and event[\"frm_name\"] == \"SPoCA\"),\n",
    "    key=lambda event: (dtf(event[\"event_starttime\"]), dtf(event[\"event_endtime\"]))\n",
    "))\n",
    "\n",
    "print(\"SPoCA regions:\", len(spoca_regions))\n",
    "\n",
    "spoca_regions_grouped = dict()\n",
    "for region in spoca_regions:\n",
    "    current_id = region[\"frm_specificid\"]\n",
    "    if current_id not in spoca_regions_grouped:\n",
    "        spoca_regions_grouped[current_id] = [region]\n",
    "    else:\n",
    "        spoca_regions_grouped[current_id].append(region)\n",
    "\n",
    "print(\"SPoCA regions (grouped):\", len(spoca_regions_grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11482 2012-05-14T00:00:00 2012-05-14T23:59:59\n",
      "11482 2012-05-15T00:00:00 2012-05-15T23:59:59\n",
      "11482 2012-05-16T00:00:00 2012-05-16T23:59:59\n",
      "11482 2012-05-17T00:00:00 2012-05-17T23:59:59\n",
      "11482 2012-05-18T00:00:00 2012-05-18T23:59:59\n",
      "11482 2012-05-19T00:00:00 2012-05-19T23:59:59\n",
      "11482 2012-05-20T00:00:00 2012-05-20T23:59:59\n",
      "11482 2012-05-21T00:00:00 2012-05-21T23:59:59\n",
      "11482 2012-05-22T00:00:00 2012-05-22T23:59:59\n",
      "11482 2012-05-23T00:00:00 2012-05-23T23:59:59\n",
      "11482 2012-05-24T00:00:00 2012-05-24T23:59:59\n",
      "\n",
      "9314 1640\n",
      "\n",
      "1304 837 468\n"
     ]
    }
   ],
   "source": [
    "noaa_regions = dict()\n",
    "\n",
    "for event in events:\n",
    "    if event[\"event_type\"] == \"AR\" and event[\"frm_name\"] == \"NOAA SWPC Observer\":\n",
    "        noaa_id = event[\"ar_noaanum\"]\n",
    "        \n",
    "        if noaa_id not in noaa_regions:\n",
    "            noaa_regions[noaa_id] = [event]\n",
    "        else:\n",
    "            noaa_regions[noaa_id].append(event)\n",
    "\n",
    "test_region = noaa_regions[11482]\n",
    "for event in test_region:\n",
    "    print(event[\"ar_noaanum\"], event[\"event_starttime\"], event[\"event_endtime\"])\n",
    "    \n",
    "print()\n",
    "\n",
    "num_fl_with_ar = 0\n",
    "num_fl_without_ar = 0\n",
    "for event in swpc_flares:\n",
    "    if event[\"ar_noaanum\"] > 0:\n",
    "        num_fl_with_ar += 1\n",
    "    else:\n",
    "        num_fl_without_ar += 1\n",
    "print(num_fl_with_ar, num_fl_without_ar)\n",
    "\n",
    "print()\n",
    "\n",
    "has_flare = {noaa_id: False for noaa_id in noaa_regions.keys()}\n",
    "\n",
    "for event in swpc_flares:\n",
    "    if event[\"ar_noaanum\"] > 0:\n",
    "        has_flare[event[\"ar_noaanum\"]] = True\n",
    "\n",
    "num_regions_with_fl = sum(has_flare.values())\n",
    "num_regions_without_fl = sum(not c for c in has_flare.values())\n",
    "\n",
    "print(len(noaa_regions), num_regions_with_fl, num_regions_without_fl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load NOAA Active Regions and SWPC Flares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_regions = dict()\n",
    "for event in events:\n",
    "    if event[\"event_type\"] == \"AR\" and event[\"frm_name\"] == \"NOAA SWPC Observer\":\n",
    "        noaa_id = event[\"ar_noaanum\"]\n",
    "        \n",
    "        if noaa_id not in noaa_regions:\n",
    "            noaa_regions[noaa_id] = [event]\n",
    "        else:\n",
    "            noaa_regions[noaa_id].append(event)\n",
    "\n",
    "noaa_regions = {\n",
    "    noaa_id: (\n",
    "        min(map(lambda event: dtf(event[\"event_starttime\"]), events)),\n",
    "        max(map(lambda event: dtf(event[\"event_endtime\"]), events)),\n",
    "        events\n",
    "    )\n",
    "    for noaa_id, events in noaa_regions.items()\n",
    "}\n",
    "\n",
    "swpc_flares = list(sorted(\n",
    "    (event for event in events if event[\"event_type\"] == \"FL\" and event[\"frm_name\"] == \"SWPC\"),\n",
    "    key=lambda event: (dtf(event[\"event_starttime\"]), dtf(event[\"event_peaktime\"]), dtf(event[\"event_endtime\"]))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Assign SWPC Flares to NOAA Active Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Assign Flares containing Active Region Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA number 12689 not present\n",
      "NOAA number 12689 not present\n",
      "Mapped flares: 9312\n",
      "Unmapped flares: 1642\n"
     ]
    }
   ],
   "source": [
    "mapped_flares = []\n",
    "unmapped_flares = []\n",
    "for event in swpc_flares:\n",
    "    region_number = event[\"ar_noaanum\"]\n",
    "    \n",
    "    if region_number > 0:\n",
    "        if region_number in noaa_regions.keys():\n",
    "            mapped_flares.append((event, region_number))\n",
    "        else:\n",
    "            print(f\"NOAA number {region_number} not present\")\n",
    "            unmapped_flares.append(event)\n",
    "    else:\n",
    "        assert region_number == 0\n",
    "        unmapped_flares.append(event)\n",
    "\n",
    "print(\"Mapped flares:\", len(mapped_flares))\n",
    "print(\"Unmapped flares:\", len(unmapped_flares))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Assign remaining Flares using Heuristics (SSW Events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA number 11383 not present\n",
      "NOAA number 12689 not present\n",
      "NOAA number 12689 not present\n",
      "Mapped flares: 10140\n",
      "Still unmapped flares: 814\n"
     ]
    }
   ],
   "source": [
    "ssw_flares = filter(lambda event: event[\"event_type\"] == \"FL\" and event[\"frm_name\"] == \"SSW Latest Events\", events)\n",
    "ssw_flares = [(\n",
    "    dtf(event[\"event_starttime\"]),\n",
    "    dtf(event[\"event_peaktime\"]),\n",
    "    dtf(event[\"event_endtime\"]),\n",
    "    event\n",
    "    ) for event in ssw_flares\n",
    "]\n",
    "\n",
    "still_unmapped_flares = []\n",
    "\n",
    "for event in unmapped_flares:\n",
    "    event_start = dtf(event[\"event_starttime\"])\n",
    "    event_end = dtf(event[\"event_endtime\"])\n",
    "    event_peak = dtf(event[\"event_peaktime\"])\n",
    "    \n",
    "    # Find matching SSW event\n",
    "    match_start, match_peak, match_end, match_event = min(\n",
    "        ssw_flares,\n",
    "        key=lambda cand: (\n",
    "            abs(event_peak - cand[1]),\n",
    "            abs(event_end - cand[2]),\n",
    "            abs(event_start - cand[0])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Check peak time delta\n",
    "    if abs(event_peak - match_peak) > dt.timedelta(minutes=1):\n",
    "        if event_start != match_start or event_end != match_end or abs(event_peak - match_peak) > dt.timedelta(minutes=10):\n",
    "            still_unmapped_flares.append(event)\n",
    "            continue\n",
    "    \n",
    "    # Check if event peak is inside match duration\n",
    "    if not (match_start <= event_peak <= match_end):\n",
    "        still_unmapped_flares.append(event)\n",
    "        continue\n",
    "\n",
    "    # Times are equal, check if NOAA id is found\n",
    "    region_number = match_event[\"ar_noaanum\"]\n",
    "\n",
    "    if region_number == 0:\n",
    "        still_unmapped_flares.append(event)\n",
    "        continue\n",
    "    \n",
    "    if event[\"fl_goescls\"] != match_event[\"fl_goescls\"]:\n",
    "        still_unmapped_flares.append(event)\n",
    "        continue\n",
    "\n",
    "    assert region_number < 10000\n",
    "    region_number += 10000\n",
    "    if region_number in noaa_regions.keys():\n",
    "        mapped_flares.append((event, region_number))\n",
    "    else:\n",
    "        print(f\"NOAA number {region_number} not present\")\n",
    "        still_unmapped_flares.append(event)\n",
    "\n",
    "print(\"Mapped flares:\", len(mapped_flares))\n",
    "print(\"Still unmapped flares:\", len(still_unmapped_flares))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Divide Active Region Time Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unmapped_ranges = intervaltree.IntervalTree()\n",
    "for event in still_unmapped_flares:\n",
    "    unmapped_start = dtf(event[\"event_starttime\"])\n",
    "    unmapped_end = dtf(event[\"event_endtime\"])\n",
    "    unmapped_ranges.addi(unmapped_start, unmapped_end)\n",
    "unmapped_ranges.merge_overlaps()\n",
    "\n",
    "DELTA_IN = dt.timedelta(hours=12)\n",
    "DELTA_OUT = dt.timedelta(hours=12)\n",
    "\n",
    "# TODO: Check edge-case handling (interval end is often inclusive but tree treats it as exclusive)\n",
    "\n",
    "ranges = dict()\n",
    "for noaa_id, (region_start, region_end, region_events) in noaa_regions.items():\n",
    "    flares = list(sorted(\n",
    "        (flare_event for flare_event, flare_region_id in mapped_flares if flare_region_id == noaa_id),\n",
    "        key=lambda flare_event: dtf(flare_event[\"event_peaktime\"])\n",
    "    ))\n",
    "    \n",
    "    free_ranges = intervaltree.IntervalTree()\n",
    "    free_ranges.addi(region_start, region_end, \"free\")\n",
    "    \n",
    "    flare_ranges = intervaltree.IntervalTree()\n",
    "    for idx, flare_event in enumerate(flares):\n",
    "        flare_peak = dtf(flare_event[\"event_peaktime\"])\n",
    "        flare_class = flare_event[\"fl_goescls\"]\n",
    "        \n",
    "        # Calculate best case range\n",
    "        range_min = max(flare_peak - DELTA_OUT, region_start + DELTA_IN)\n",
    "        range_max = min(flare_peak + DELTA_OUT, region_end)\n",
    "        \n",
    "        # Check if any bigger flares happen before\n",
    "        prev_idx = idx - 1\n",
    "        while prev_idx >= 0 and dtf(flares[prev_idx][\"event_peaktime\"]) > range_min:\n",
    "            # Check if check flare is bigger than current, if yes, reduce start range\n",
    "            if flares[prev_idx][\"fl_goescls\"] > flare_class:\n",
    "                range_min = dtf(flares[prev_idx][\"event_peaktime\"])\n",
    "                break\n",
    "            else:\n",
    "                prev_idx -= 1\n",
    "\n",
    "        # Check if any bigger flares happen afterwards\n",
    "        next_idx = idx + 1\n",
    "        while next_idx < len(flares) and dtf(flares[next_idx][\"event_peaktime\"]) < range_max:\n",
    "            # Check if check flare is bigger than current, if yes, reduce start range\n",
    "            if flares[next_idx][\"fl_goescls\"] > flare_class:\n",
    "                range_max = dtf(flares[next_idx][\"event_peaktime\"])\n",
    "                break\n",
    "            else:\n",
    "                next_idx += 1\n",
    "        \n",
    "        if range_max - range_min >= DELTA_OUT:\n",
    "            assert range_min <= flare_peak <= range_max\n",
    "            assert region_start <= flare_peak <= region_end\n",
    "            assert range_min < range_max\n",
    "            assert range_min - DELTA_IN >= region_start\n",
    "            \n",
    "            flare_ranges.addi(range_min, range_max, flare_class)\n",
    "\n",
    "        # Remove range around flare from free areas\n",
    "        free_ranges.chop(flare_peak - DELTA_OUT, flare_peak + DELTA_OUT)\n",
    "\n",
    "    # Merge free and flare ranges\n",
    "    region_ranges = free_ranges | flare_ranges\n",
    "    \n",
    "    # Remove unmapped ranges from result\n",
    "    for current_chop_interval in unmapped_ranges:\n",
    "        region_ranges.chop(*current_chop_interval)\n",
    "        \n",
    "        if len(region_ranges[current_chop_interval]) > 0:\n",
    "            # This seems to be a bug in the library where the chop operation does nothing\n",
    "            # Creating a new tree seems to fix it\n",
    "            region_ranges = intervaltree.IntervalTree(set(region_ranges))\n",
    "            region_ranges.chop(*current_chop_interval)\n",
    "        \n",
    "        assert len(region_ranges[current_chop_interval]) == 0\n",
    "    \n",
    "    # Remove ranges which are too small\n",
    "    intervals_remove = [interval for interval in region_ranges if interval.end - interval.begin < DELTA_OUT]\n",
    "    for current_interval in intervals_remove:\n",
    "        assert current_interval in region_ranges\n",
    "        region_ranges.discardi(*current_interval)\n",
    "        assert current_interval not in region_ranges\n",
    "\n",
    "    ranges[noaa_id] = region_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noaa_id, region_ranges in ranges.items():\n",
    "    # Check length is valid\n",
    "    assert not any(interval.end - interval.begin < DELTA_OUT for interval in region_ranges)\n",
    "    \n",
    "    # Check no overlaps between free regions\n",
    "    sorted_intervals = list(sorted(interval for interval in region_ranges if interval.data == \"free\"))\n",
    "    for idx, interval in enumerate(sorted_intervals):\n",
    "        # Check free regions within themselfs\n",
    "        assert idx == 0 or interval.begin > sorted_intervals[idx - 1].end, \\\n",
    "            f\"Found overlap between free intervals {interval} and {sorted_intervals[idx - 1]}\"\n",
    "        \n",
    "        # Check free regions and flares\n",
    "        assert len(region_ranges[interval]) == 1\n",
    "    \n",
    "    # Check distance between flare ranges\n",
    "    sorted_flare_intervals = list(sorted(interval for interval in region_ranges if interval.data != \"free\"))\n",
    "    for idx, interval in enumerate(sorted_flare_intervals):\n",
    "        assert idx == 0 or interval.begin + DELTA_OUT >= sorted_flare_intervals[idx - 1].end or interval.data <= sorted_flare_intervals[idx - 1].data, \\\n",
    "            f\"Found overlap between flares {interval} and {sorted_flare_intervals[idx - 1]}\"\n",
    "\n",
    "    # Check no unmapped flares are overlapped\n",
    "    for interval in region_ranges:\n",
    "        assert len(unmapped_ranges[interval]) == 0, \\\n",
    "            f\"{interval} overlaps unmapped flare ranges {unmapped_ranges[interval]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Some active regions might still produce flares which are not detected by SWPC\n",
    "# TODO: Make sure no instrument issues are present (e.g. satellite maneuvers)\n",
    "# TODO: Could also use SPoCA ARs to get way more data\n",
    "# TODO: Check if active regions overlap each other to avoid duplicates\n",
    "# TODO: Check if SSW data is actually reliable\n",
    "# TODO: Can we really use arbitrary data if DELTA_IN > 0? (Except for checking if the complete AR is visible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'fl': 5395, 'free': 2783})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ranges_classes = collections.defaultdict(int)\n",
    "\n",
    "for region_ranges in ranges.values():\n",
    "    for current_range in region_ranges:\n",
    "        current_class = \"fl\" if current_range.data != \"free\" else current_range.data\n",
    "        \n",
    "        num_ranges_classes[current_class] += 1\n",
    "\n",
    "num_ranges_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
